{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75524107-ecf6-49de-b06f-1834c9de85ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c76f9d6-477b-43a4-aef7-7ebb6f3d1ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c612f4-e6f2-4c45-8232-3bc12cd8e271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac21d5-ca7f-43e7-956d-83d8685049fe",
   "metadata": {},
   "source": [
    "## Overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da66ebe-60e0-44d9-9a06-852be1eb315f",
   "metadata": {},
   "source": [
    "- HuggingFace bitsandbytes\n",
    "- GPTQ: data **compression**, GPU，https://arxiv.org/pdf/2210.17323\n",
    "    - GPTQ is a **post-training quantization (PTQ)** method for 4-bit quantization that focuses primarily on **GPU** inference and performance.\n",
    "    - to quantizing the weights of transformer-based models\n",
    "    - first applies scalar quant to the weights, followed by vector quant to the residuals\n",
    "    - The idea behind the method is that it will try to **compress all weights to a 4-bit quantization** by minimizing the **mean squared error** to that weight.\n",
    "        - During inference, it will dynamically dequantize its weights to float16 for improved performance whilst keeping memory low.\n",
    "- GGUF: ggml, CPU\n",
    "    - c++, \n",
    "    - llama.cpp, https://github.com/ggerganov/llama.cpp\n",
    "- AWQ：activation aware quantization，https://arxiv.org/abs/2306.00978"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccef1ea-ef4e-4393-9a96-3d164d763867",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f98b2f3-72aa-44b7-a4e1-56e719a1a292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084289a2a83d419d8fb0b5687522ede0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in your LLM without any compression tricks\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\" \n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d0f04e-910d-4268-b347-a8d89dd732db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "025925bc-df5e-4227-8fc5-43aa99c6dbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q_proj = pipe.model.model.layers[0].self_attn.q_proj.weight.detach().to(torch.float16).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb2d327-d751-4ca2-90fe-a70c09bdef2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "# 可以发现基本是集中在均值 0 附近\n",
    "sns.histplot(q_proj[:10000], bins=50, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c197f-0de6-4c0f-bc3d-9113fcc08936",
   "metadata": {},
   "source": [
    "## Chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730297c-d16f-4503-9dd8-88b8622e3870",
   "metadata": {},
   "source": [
    "- llama3\n",
    "    - `<|begin_of_text|>`\n",
    "    - `<|start_header_id|>system<|end_header_id|>....<|eot_id|>`\n",
    "    - `<|start_header_id|>user<|end_header_id|>...<|eot_id|>`\n",
    "    - `<|start_header_id|>assistant<|end_header_id|>...`\n",
    "- zephyr（部分其他模型）\n",
    "    - `<|system|> ... </s>`\n",
    "    - `<|user|> ... </s>`\n",
    "    - `<|assistant|> ... </s>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e1f09d-eb7b-400f-85b3-c31ac0ebad48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ee927c-6231-48c8-b264-88951ae53590",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='meta-llama/Meta-Llama-3-8B-Instruct', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128004: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128005: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128008: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128010: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128011: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128012: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128013: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128014: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128015: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128016: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128017: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128018: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128019: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128020: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128021: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128022: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128023: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128024: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128025: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128026: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128027: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128028: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128029: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128030: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128031: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128032: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128033: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128034: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128035: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128036: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128037: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128038: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128039: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128040: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128041: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128042: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128043: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128044: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128045: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128046: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128047: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128048: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128049: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128050: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128051: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128052: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128053: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128054: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128055: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128056: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128057: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128058: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128059: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128060: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128061: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128062: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128063: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128064: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128065: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128066: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128067: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128068: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128069: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128070: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128071: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128072: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128073: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128074: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128075: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128076: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128077: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128078: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128079: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128080: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128081: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128082: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128083: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128084: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128085: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128086: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128087: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128088: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128089: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128090: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128091: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128092: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128093: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128094: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128095: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128096: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128097: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128098: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128099: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128100: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128101: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128102: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128103: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128104: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128105: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128106: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128107: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128108: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128109: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128110: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128111: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128112: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128113: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128114: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128115: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128116: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128117: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128118: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128119: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128120: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128121: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128122: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128123: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128124: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128125: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128126: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128127: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128128: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128129: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128130: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128131: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128132: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128133: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128134: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128135: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128136: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128137: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128138: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128139: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128140: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128141: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128142: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128143: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128144: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128145: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128146: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128147: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128148: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128149: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128150: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128151: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128152: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128153: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128154: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128155: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128156: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128157: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128158: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128159: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128160: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128161: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128162: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128163: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128164: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128165: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128166: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128167: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128168: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128169: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128170: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128171: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128172: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128173: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128174: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128175: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128176: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128177: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128178: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128179: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128180: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128181: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128182: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128183: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128184: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128185: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128186: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128187: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128188: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128189: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128190: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128191: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128192: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128193: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128194: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128195: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128196: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128197: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128198: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128199: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128200: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128201: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128202: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128203: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128204: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128205: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128206: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128207: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128208: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128209: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128210: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128211: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128212: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128213: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128214: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128215: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128216: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128217: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128218: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128219: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128220: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128221: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128222: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128223: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128224: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128225: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128226: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128227: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128228: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128229: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128230: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128231: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128232: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128233: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128234: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128235: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128236: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128237: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128238: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128239: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128240: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128241: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128242: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128243: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128244: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128245: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128246: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128247: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128248: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128249: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128250: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128251: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128252: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128253: AddedToken(\"<|reserved_special_token_248|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128254: AddedToken(\"<|reserved_special_token_249|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128255: AddedToken(\"<|reserved_special_token_250|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = AutoTokenizer.from_pretrained(model_id)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "067aab63-3272-461c-9c22-75b3ecad92dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 27, 91, 9125, 91, 29]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.encode('<|system|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d49adc64-edc8-4141-9b09-bb60838d99e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"imgs/prompt_template.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='imgs/prompt_template.png', width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a57a64-a96b-4786-acf3-2fadcd5dd992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b0b47-60bf-4bbe-b312-1e71e08c8f34",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9da8c78a-f8f4-4ad9-bff0-7a2658409aab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52988904-08a2-40bc-b3c5-f7d00a2e1add",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.988792896270752"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated(device='cuda:0') / (1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfe6aaae-38dc-4fcd-8123-9850bd134ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Why did the Large Language Model go to therapy?\n",
      "\n",
      "Because it was feeling a little \"dis-connected\" and was struggling to \"process\" its emotions!\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cdb14-1152-44a9-9efd-5346cfd2b196",
   "metadata": {},
   "source": [
    "## Sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dab17c1-3175-4b8f-858f-d2363c190ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NCCL_P2P_DISABLE'] = \"1\"\n",
    "os.environ['NCCL_IB_DISABLE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f586c8c4-48ed-4dfe-ab9f-c1c470ef43c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-13 14:01:00,907] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/miniconda3/envs/nlp_study/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/miniconda3/envs/nlp_study/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/root/miniconda3/envs/nlp_study/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/root/miniconda3/envs/nlp_study/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/root/miniconda3/envs/nlp_study/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Shard our model into pieces\n",
    "accelerator = Accelerator()\n",
    "accelerator.save_model(\n",
    "    model=pipe.model,\n",
    "    save_directory=\"./model\",\n",
    "    max_shard_size=\"4GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01cd3e-d947-4b67-8fb0-75ce44b1cef0",
   "metadata": {},
   "source": [
    "## Quant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8f8ce-1d9c-4687-b5b8-1be459d96a4d",
   "metadata": {},
   "source": [
    "- 4bit-NormalFloat (NF4, **qlora: lora on a quantize LLMs**， https://arxiv.org/abs/2305.14314) consists of three steps:\n",
    "    - Normalization: The weights of the model are normalized so that we expect the weights to fall within **a certain range**. This allows for more efficient representation of more common values.\n",
    "        - The weights of the model are **first normalized to have zero mean and unit variance**. This ensures that the weights are **distributed around zero** and fall within a certain range.\n",
    "    - Quantization: The weights are **quantized to 4-bit**. In NF4, the quantization levels are evenly spaced with respect to the normalized weights, thereby efficiently representing the original 32-bit weights.\n",
    "        - The normalized weights are then quantized to 4 bits. This involves mapping the original high-precision weights to a smaller set of low-precision values. In the case of NF4, the quantization levels are chosen to be **evenly spaced** in the range of the normalized weights.\n",
    "    - Dequantization: Although the weights are stored in 4-bit, they are dequantized during computation which gives a performance boost during inference.\n",
    "        - During the forward pass and backpropagation, the quantized weights are **dequantized back to full precision**. This is done by mapping the 4-bit quantized values back to their original range. The dequantized weights are used in the computations, but they are stored in memory in their 4-bit quantized form.\n",
    "- bitsandbytes 的分位数计算\n",
    "    - 密度高的地方多分配，密度低的地方少分配\n",
    "    - https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/bitsandbytes/functional.py#L267\n",
    "    - https://zhuanlan.zhihu.com/p/647378373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "520d6c3d-46fa-490e-9552-5d54f9a5a43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"imgs/data_precision.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='imgs/data_precision.png', width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "818a5f48-18a3-40f4-98f8-273a5e268a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"imgs/data_precision_example.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='imgs/data_precision_example.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55f12041-dcac-4e75-b8f3-793689f72e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0.1234, 75535.0000]) torch.float32\n",
      "tensor([0.1234,    inf], dtype=torch.float16)\n",
      "tensor([    0.1235, 75776.0000], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "X = torch.tensor([0.1234, 75535])\n",
    "\n",
    "print(X, X.dtype)  # 默认为 float32\n",
    "print(X.to(torch.float16))\n",
    "print(X.to(torch.bfloat16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51308f97-d07a-4b32-8190-d9805157a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any models previously created\n",
    "# del pipe, accelerator\n",
    "\n",
    "# Empty VRAM cache\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2466017-b83b-42f4-b920-d59bdafbe02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from torch import bfloat16\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\" \n",
    "\n",
    "# Our 4-bit configuration to load the LLM with less GPU memory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,               # 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',       # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
    "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d15eb9-4f9e-4ceb-a566-7a1cbe4487c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3718a6863209455ab64f6a7d175ee4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With BitsAndBytes Configuration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f50ca8-f1ee-49e4-86c5-048316c2a290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.42464542388916"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated('cuda:0') / (1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdbba1fc-083f-4a52-8f83-2cec9402b84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c024db25-82a8-414a-a27a-08eeab85ec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a funny joke about Large Language Models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Why did the Large Language Model go to therapy?\n",
      "\n",
      "Because it was struggling to \"process\" its emotions and was feeling a little \"disconnected\"!\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e407eca-2caa-4336-b59e-df4ea4a93dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.544897556304932"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated('cuda:0') / (1024*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a20ab8-d237-4236-8507-83293cda3f6c",
   "metadata": {},
   "source": [
    "注意，这里的量化是不完全量化。\n",
    "\n",
    "例如，load_in_4bit：\n",
    "- embed_tokens 继续是 torch.float16\n",
    "- 每个 layer 的内部（self attention）以及 mlp 部分是 uint8\n",
    "- 每个 layer 的 output（layernorm）部分是 float16（如果 load 时传入了 `torch_dtype=torch.bfloat16`，则这部分为 torch.bfloat16）\n",
    "- 同理适用于 load_in_8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af14f3a-1003-4f10-a86d-7954e4d85b10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.float16 torch.Size([128256, 4096]) cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.0.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.0.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.0.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.1.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.1.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.1.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.2.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.2.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.2.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.3.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.3.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.3.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.4.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.4.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.4.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.5.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.5.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.5.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.6.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.6.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.6.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.7.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.7.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.7.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.8.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.8.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.8.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.8.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.8.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.8.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.8.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.8.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.8.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.9.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.9.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.9.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.9.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.9.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.9.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.9.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.9.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.9.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.10.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.10.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.10.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.10.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.10.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.10.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.10.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.10.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.10.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.11.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.11.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.11.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.11.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.11.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.11.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.11.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.11.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.11.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.12.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.12.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.12.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.12.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.12.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.12.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.12.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.12.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.12.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.13.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.13.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.13.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.13.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.13.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.13.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.13.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.13.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.13.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.14.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.14.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.14.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.14.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.14.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.14.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.14.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.14.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.14.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.15.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.15.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.15.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.15.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.15.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.15.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.15.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.15.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.15.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.16.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.16.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.16.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.16.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.16.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.16.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.16.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.16.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.16.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.17.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.17.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.17.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.17.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.17.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.17.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.17.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.17.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.17.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.18.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.18.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.18.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.18.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.18.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.18.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.18.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.18.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.18.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.19.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.19.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.19.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.19.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.19.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.19.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.19.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.19.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.19.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.20.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.20.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.20.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.20.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.20.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.20.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.20.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.20.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.20.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.21.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.21.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.21.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.21.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.21.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.21.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.21.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.21.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.21.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.22.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.22.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.22.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.22.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.22.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.22.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.22.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.22.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.22.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.23.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.23.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.23.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.23.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.23.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.23.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.23.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.23.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.23.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.24.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.24.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.24.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.24.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.24.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.24.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.24.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.24.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.24.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.25.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.25.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.25.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.25.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.25.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.25.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.25.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.25.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.25.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.26.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.26.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.26.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.26.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.26.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.26.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.26.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.26.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.26.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.27.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.27.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.27.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.27.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.27.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.27.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.27.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.27.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.27.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.28.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.28.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.28.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.28.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.28.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.28.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.28.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.28.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.28.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.29.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.29.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.29.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.29.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.29.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.29.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.29.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.29.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.29.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.30.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.30.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.30.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.30.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.30.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.30.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.30.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.30.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.30.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.31.self_attn.q_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.31.self_attn.k_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.31.self_attn.v_proj.weight torch.uint8 torch.Size([2097152, 1]) cuda:0\n",
      "model.layers.31.self_attn.o_proj.weight torch.uint8 torch.Size([8388608, 1]) cuda:0\n",
      "model.layers.31.mlp.gate_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.31.mlp.up_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.31.mlp.down_proj.weight torch.uint8 torch.Size([29360128, 1]) cuda:0\n",
      "model.layers.31.input_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.layers.31.post_attention_layernorm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "model.norm.weight torch.float16 torch.Size([4096]) cuda:0\n",
      "lm_head.weight torch.float16 torch.Size([128256, 4096]) cuda:0\n"
     ]
    }
   ],
   "source": [
    "for name, para in pipe.model.named_parameters():\n",
    "    print(name, para.dtype, para.shape, para.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cf2bd-0db5-4699-a542-ab67131ec7a1",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580480ac-63bc-4898-a83d-8bfee2b47d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any models previously created\n",
    "del tokenizer, model, pipe\n",
    "\n",
    "# Empty VRAM cache\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcd353-5953-4dc2-b2ea-613131f3e743",
   "metadata": {},
   "source": [
    "- https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ\n",
    "- https://github.com/AutoGPTQ/AutoGPTQ （可以走源码安装）\n",
    "\n",
    "```\n",
    "# GPTQ Dependencies\n",
    "# !pip install optimum\n",
    "# !pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf282d7-0d72-46b7-af3f-e9f9c287787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load LLM and Tokenizer\n",
    "model_id = \"MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35f25e-5dfc-4fcb-9dd4-da89aaac76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8481b0-15d6-46c7-9a8c-884a8b81292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f982d7-0825-4d2b-827f-89b56781d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.max_memory_allocated('cuda:0') / (1024*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05213867-b8e0-4802-b10e-0e7f87282a90",
   "metadata": {},
   "source": [
    "## GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8ca34-5496-4a95-b4dd-ba511f8ef78d",
   "metadata": {},
   "source": [
    "- GPT-Generated Unified Format，是由Georgi Gerganov定义发布的一种大模型文件格式。Georgi Gerganov是著名开源项目llama.cpp的创始人。\n",
    "    - GGML：GPT-Generated Model Language\n",
    "- Although GPTQ does compression well, its **focus on GPU** can be a disadvantage if you do not have the hardware to run it.\n",
    "    - GGUF, previously GGML, is a quantization method that allows users to **use the CPU to run an LLM** but also **offload some of its layers to the GPU** for a speed up (`llama.cpp` 中的 `-ngl` ). Although using the CPU is generally slower than using a GPU for inference, it is an incredible format for those running models on CPU or Apple devices.\n",
    "    - Especially since we are seeing smaller and more capable models appearing, like Mistral 7B, the GGUF format might just be here to stay!\n",
    "- Q4_K_M\n",
    "    - Q stands for Quantization.\n",
    "    - 4 indicates the number of bits used in the quantization process.\n",
    "    - K refers to the use of **k-means** clustering in the quantization.\n",
    "    - M represents the size of the model after quantization.\n",
    "        - (S = Small, M = Medium, L = Large)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc056d14-852f-4cbf-a468-0f6f972f6e4a",
   "metadata": {},
   "source": [
    "### quant base kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e4c830-7f7b-489d-b2bd-e85e0487eed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"imgs/quant-clustering.jpeg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='imgs/quant-clustering.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2e315-431f-40ad-8f4f-8cf9483940ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 原始权重矩阵\n",
    "weights = np.array([\n",
    "    [2.09, -0.98, 1.48, 0.09],\n",
    "    [0.05, -0.14, -1.08, 2.12],\n",
    "    [-0.91, 1.92, 0, -1.03],\n",
    "    [1.87, 0, 1.53, 1.49]\n",
    "])\n",
    "\n",
    "# K-means聚类\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(weights.reshape(-1, 1))\n",
    "cluster_indices = kmeans.predict(weights.reshape(-1, 1)).reshape(weights.shape)\n",
    "centroids = kmeans.cluster_centers_.flatten()\n",
    "\n",
    "# 根据质心值排序\n",
    "sorted_indices = np.argsort(centroids)\n",
    "sorted_centroids = centroids[sorted_indices]\n",
    "\n",
    "# 创建索引映射\n",
    "index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(sorted_indices)}\n",
    "\n",
    "# 更新量化索引矩阵\n",
    "new_cluster_indices = np.vectorize(index_map.get)(cluster_indices)\n",
    "\n",
    "print(\"重新排序后的量化索引矩阵：\\n\", new_cluster_indices)\n",
    "print(\"重新排序后的质心值：\\n\", sorted_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1efcb7-5fbf-477e-bac1-a83394963aba",
   "metadata": {},
   "source": [
    "### gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb318a-8403-4f55-916f-8cd94964d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer, model, pipe\n",
    "\n",
    "# Empty VRAM cache\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b1001-95cd-4094-9480-6818b072d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load LLM and Tokenizer\n",
    "# Use `gpu_layers` to specify how many layers will be offloaded to the GPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "    model_file=\"Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\",\n",
    "    # model_type=\"llama\", \n",
    "    gpu_layers=20, hf=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF\", use_fast=True\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b01fb-703a-49f8-bf5f-a0ec270e02a3",
   "metadata": {},
   "source": [
    "## AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af6cef-d624-4015-9f92-5735786370bb",
   "metadata": {},
   "source": [
    "\n",
    "A new format on the block is AWQ ([Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978)) which is a quantization method similar to GPTQ. There are several differences between AWQ and GPTQ as methods but the most important one is that AWQ assumes that **not all weights are equally important for an LLM's performance**.\n",
    "\n",
    "In other words, there is a small fraction of weights that will be skipped during quantization which helps with the quantization loss.\n",
    "\n",
    "As a result, their paper mentions a **significant speed-up compared to GPTQ whilst keeping similar, and sometimes even better, performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5cf169-49b4-41dc-9965-22840ca542a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Load the LLM\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=256)\n",
    "llm = LLM(\n",
    "    model=\"casperhansen/llama-3-8b-instruct-awq\",\n",
    "    quantization='awq',\n",
    "    dtype='half',\n",
    "    gpu_memory_utilization=.95,\n",
    "    max_model_len=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f141bc-4f6d-4de9-8dba-b1f5d707d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3-8b-instruct-awq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db154c-ea44-449a-ab57-e980b4c5cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff08c2-d166-4e6f-926e-2466ab8cc186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate output based on the input prompt and sampling parameters\n",
    "output = llm.generate(prompt, sampling_params)\n",
    "print(output[0].outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_study",
   "language": "python",
   "name": "nlp_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
