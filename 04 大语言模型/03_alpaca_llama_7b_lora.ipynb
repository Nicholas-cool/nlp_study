{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9983e69-8cdf-417a-a4a8-8d9029acedb2",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab68d5c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:06:43.705541Z",
     "start_time": "2023-06-09T15:06:43.374754Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184b1ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:06:46.829655Z",
     "start_time": "2023-06-09T15:06:46.819209Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.45.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c0c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42597993",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- hf（huggingface）中使用 llama\n",
    "- llama => alpaca\n",
    "- lora on alpaca\n",
    "- inference：推理\n",
    "    - alpaca 标准 prompt 格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a016b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load model/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e387249",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- https://github.com/tloen/alpaca-lora/blob/main/generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b17cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:07:27.344536Z",
     "start_time": "2023-06-09T15:07:25.495233Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1b2166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T14:18:57.287680Z",
     "start_time": "2023-06-01T14:18:57.282774Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc02ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123ea7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:08:45.956768Z",
     "start_time": "2023-06-09T15:08:33.034604Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea53a5c827f142baa9e0415739bcec29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_path = \"../../autodl-fs/model_path\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"yahma/llama-7b-hf\",\n",
    "    load_in_8bit=True,    # 导致混合精度\n",
    "    device_map=\"auto\",    # 自动模型并行\n",
    "    cache_dir=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d77948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:08:58.988075Z",
     "start_time": "2023-06-09T15:08:58.978500Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d4fd5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:10:13.236567Z",
     "start_time": "2023-06-09T15:10:13.219727Z"
    },
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, model.embed_tokens.weight\t cuda:0 \ttorch.float16\n",
      "1, model.layers.0.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "2, model.layers.0.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "3, model.layers.0.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "4, model.layers.0.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "5, model.layers.0.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "6, model.layers.0.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "7, model.layers.0.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "8, model.layers.0.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "9, model.layers.0.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "10, model.layers.1.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "11, model.layers.1.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "12, model.layers.1.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "13, model.layers.1.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "14, model.layers.1.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "15, model.layers.1.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "16, model.layers.1.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "17, model.layers.1.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "18, model.layers.1.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "19, model.layers.2.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "20, model.layers.2.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "21, model.layers.2.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "22, model.layers.2.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "23, model.layers.2.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "24, model.layers.2.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "25, model.layers.2.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "26, model.layers.2.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "27, model.layers.2.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "28, model.layers.3.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "29, model.layers.3.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "30, model.layers.3.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "31, model.layers.3.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "32, model.layers.3.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "33, model.layers.3.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "34, model.layers.3.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "35, model.layers.3.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "36, model.layers.3.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "37, model.layers.4.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "38, model.layers.4.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "39, model.layers.4.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "40, model.layers.4.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "41, model.layers.4.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "42, model.layers.4.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "43, model.layers.4.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "44, model.layers.4.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "45, model.layers.4.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "46, model.layers.5.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "47, model.layers.5.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "48, model.layers.5.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "49, model.layers.5.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "50, model.layers.5.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "51, model.layers.5.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "52, model.layers.5.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "53, model.layers.5.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "54, model.layers.5.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "55, model.layers.6.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "56, model.layers.6.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "57, model.layers.6.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "58, model.layers.6.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "59, model.layers.6.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "60, model.layers.6.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "61, model.layers.6.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "62, model.layers.6.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "63, model.layers.6.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "64, model.layers.7.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "65, model.layers.7.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "66, model.layers.7.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "67, model.layers.7.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "68, model.layers.7.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "69, model.layers.7.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "70, model.layers.7.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "71, model.layers.7.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "72, model.layers.7.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "73, model.layers.8.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "74, model.layers.8.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "75, model.layers.8.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "76, model.layers.8.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "77, model.layers.8.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "78, model.layers.8.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "79, model.layers.8.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "80, model.layers.8.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "81, model.layers.8.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "82, model.layers.9.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "83, model.layers.9.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "84, model.layers.9.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "85, model.layers.9.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "86, model.layers.9.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "87, model.layers.9.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "88, model.layers.9.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "89, model.layers.9.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "90, model.layers.9.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "91, model.layers.10.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "92, model.layers.10.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "93, model.layers.10.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "94, model.layers.10.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "95, model.layers.10.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "96, model.layers.10.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "97, model.layers.10.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "98, model.layers.10.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "99, model.layers.10.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "100, model.layers.11.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "101, model.layers.11.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "102, model.layers.11.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "103, model.layers.11.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "104, model.layers.11.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "105, model.layers.11.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "106, model.layers.11.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "107, model.layers.11.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "108, model.layers.11.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "109, model.layers.12.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "110, model.layers.12.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "111, model.layers.12.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "112, model.layers.12.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "113, model.layers.12.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "114, model.layers.12.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "115, model.layers.12.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "116, model.layers.12.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "117, model.layers.12.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "118, model.layers.13.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "119, model.layers.13.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "120, model.layers.13.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "121, model.layers.13.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "122, model.layers.13.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "123, model.layers.13.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "124, model.layers.13.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "125, model.layers.13.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "126, model.layers.13.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "127, model.layers.14.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "128, model.layers.14.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "129, model.layers.14.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "130, model.layers.14.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "131, model.layers.14.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "132, model.layers.14.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "133, model.layers.14.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "134, model.layers.14.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "135, model.layers.14.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "136, model.layers.15.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "137, model.layers.15.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "138, model.layers.15.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "139, model.layers.15.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "140, model.layers.15.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "141, model.layers.15.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "142, model.layers.15.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "143, model.layers.15.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "144, model.layers.15.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "145, model.layers.16.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "146, model.layers.16.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "147, model.layers.16.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "148, model.layers.16.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "149, model.layers.16.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "150, model.layers.16.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "151, model.layers.16.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "152, model.layers.16.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "153, model.layers.16.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "154, model.layers.17.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "155, model.layers.17.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "156, model.layers.17.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "157, model.layers.17.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "158, model.layers.17.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "159, model.layers.17.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "160, model.layers.17.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "161, model.layers.17.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "162, model.layers.17.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "163, model.layers.18.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "164, model.layers.18.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "165, model.layers.18.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "166, model.layers.18.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "167, model.layers.18.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "168, model.layers.18.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "169, model.layers.18.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "170, model.layers.18.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "171, model.layers.18.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "172, model.layers.19.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "173, model.layers.19.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "174, model.layers.19.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "175, model.layers.19.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "176, model.layers.19.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "177, model.layers.19.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "178, model.layers.19.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "179, model.layers.19.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "180, model.layers.19.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "181, model.layers.20.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "182, model.layers.20.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "183, model.layers.20.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "184, model.layers.20.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "185, model.layers.20.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "186, model.layers.20.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "187, model.layers.20.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "188, model.layers.20.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "189, model.layers.20.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "190, model.layers.21.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "191, model.layers.21.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "192, model.layers.21.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "193, model.layers.21.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "194, model.layers.21.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "195, model.layers.21.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "196, model.layers.21.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "197, model.layers.21.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "198, model.layers.21.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "199, model.layers.22.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "200, model.layers.22.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "201, model.layers.22.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "202, model.layers.22.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "203, model.layers.22.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "204, model.layers.22.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "205, model.layers.22.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "206, model.layers.22.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "207, model.layers.22.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "208, model.layers.23.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "209, model.layers.23.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "210, model.layers.23.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "211, model.layers.23.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "212, model.layers.23.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "213, model.layers.23.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "214, model.layers.23.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "215, model.layers.23.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "216, model.layers.23.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "217, model.layers.24.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "218, model.layers.24.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "219, model.layers.24.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "220, model.layers.24.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "221, model.layers.24.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "222, model.layers.24.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "223, model.layers.24.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "224, model.layers.24.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "225, model.layers.24.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "226, model.layers.25.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "227, model.layers.25.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "228, model.layers.25.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "229, model.layers.25.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "230, model.layers.25.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "231, model.layers.25.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "232, model.layers.25.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "233, model.layers.25.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "234, model.layers.25.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "235, model.layers.26.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "236, model.layers.26.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "237, model.layers.26.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "238, model.layers.26.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "239, model.layers.26.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "240, model.layers.26.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "241, model.layers.26.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "242, model.layers.26.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "243, model.layers.26.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "244, model.layers.27.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "245, model.layers.27.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "246, model.layers.27.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "247, model.layers.27.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "248, model.layers.27.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "249, model.layers.27.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "250, model.layers.27.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "251, model.layers.27.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "252, model.layers.27.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "253, model.layers.28.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "254, model.layers.28.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "255, model.layers.28.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "256, model.layers.28.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "257, model.layers.28.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "258, model.layers.28.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "259, model.layers.28.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "260, model.layers.28.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "261, model.layers.28.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "262, model.layers.29.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "263, model.layers.29.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "264, model.layers.29.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "265, model.layers.29.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "266, model.layers.29.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "267, model.layers.29.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "268, model.layers.29.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "269, model.layers.29.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "270, model.layers.29.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "271, model.layers.30.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "272, model.layers.30.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "273, model.layers.30.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "274, model.layers.30.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "275, model.layers.30.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "276, model.layers.30.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "277, model.layers.30.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "278, model.layers.30.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "279, model.layers.30.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "280, model.layers.31.self_attn.q_proj.weight\t cuda:0 \ttorch.int8\n",
      "281, model.layers.31.self_attn.k_proj.weight\t cuda:0 \ttorch.int8\n",
      "282, model.layers.31.self_attn.v_proj.weight\t cuda:0 \ttorch.int8\n",
      "283, model.layers.31.self_attn.o_proj.weight\t cuda:0 \ttorch.int8\n",
      "284, model.layers.31.mlp.gate_proj.weight\t cuda:0 \ttorch.int8\n",
      "285, model.layers.31.mlp.up_proj.weight\t cuda:0 \ttorch.int8\n",
      "286, model.layers.31.mlp.down_proj.weight\t cuda:0 \ttorch.int8\n",
      "287, model.layers.31.input_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "288, model.layers.31.post_attention_layernorm.weight\t cuda:0 \ttorch.float16\n",
      "289, model.norm.weight\t cuda:0 \ttorch.float16\n",
      "290, lm_head.weight\t cuda:0 \ttorch.float16\n"
     ]
    }
   ],
   "source": [
    "for i, para in enumerate(model.named_parameters()):\n",
    "    print(f'{i}, {para[0]}\\t {para[1].device} \\t{para[1].dtype}')\n",
    "    # print(f'{i}, \\t {para[1].device} \\t{para[1].dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea34330",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea48ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:11:12.408226Z",
     "start_time": "2023-06-09T15:11:12.151520Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"yahma/llama-7b-hf\", cache_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d1ebd6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:11:20.203567Z",
     "start_time": "2023-06-09T15:11:20.193320Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='yahma/llama-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2ebe5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## With lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1cc604",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- https://github.com/tloen/alpaca-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb0dc85d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:11:55.054789Z",
     "start_time": "2023-06-09T15:11:55.015703Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e9de4-9400-49ef-8a2d-48842e80f3e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "下面代码如果遇到 ` __init__() got an unexpected keyword argument 'enable_lora'` 问题，是由于新版本的 peft 不再支持旧版本配置中的部分字段。需要手动打开模型的 `adapter_config.json` 文件，删除 `\"enable_lora\": null,` 这行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c26cfe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:13:10.960592Z",
     "start_time": "2023-06-09T15:12:54.992207Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc27b4bf4af47d3b2e999f9e8992715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/67.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, \"yahma/alpaca-7b-lora\", cache_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e40af9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:13:13.139937Z",
     "start_time": "2023-06-09T15:13:13.130339Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "307b4e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:13:52.070605Z",
     "start_time": "2023-06-09T15:13:52.055864Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type llama\n",
      "{'v_proj', 'k_proj', 'q_proj', 'o_proj'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'t5': ['q', 'v'],\n",
       " 'mt5': ['q', 'v'],\n",
       " 'bart': ['q_proj', 'v_proj'],\n",
       " 'gpt2': ['c_attn'],\n",
       " 'bloom': ['query_key_value'],\n",
       " 'blip-2': ['q', 'v', 'q_proj', 'v_proj'],\n",
       " 'opt': ['q_proj', 'v_proj'],\n",
       " 'gptj': ['q_proj', 'v_proj'],\n",
       " 'gpt_neox': ['query_key_value'],\n",
       " 'gpt_neo': ['q_proj', 'v_proj'],\n",
       " 'bert': ['query', 'value'],\n",
       " 'roberta': ['query', 'value'],\n",
       " 'xlm-roberta': ['query', 'value'],\n",
       " 'electra': ['query', 'value'],\n",
       " 'deberta-v2': ['query_proj', 'value_proj'],\n",
       " 'deberta': ['in_proj'],\n",
       " 'layoutlm': ['query', 'value'],\n",
       " 'llama': ['q_proj', 'v_proj'],\n",
       " 'chatglm': ['query_key_value'],\n",
       " 'gpt_bigcode': ['c_attn'],\n",
       " 'mpt': ['Wqkv'],\n",
       " 'RefinedWebModel': ['query_key_value'],\n",
       " 'RefinedWeb': ['query_key_value'],\n",
       " 'falcon': ['query_key_value'],\n",
       " 'btlm': ['c_proj', 'c_attn'],\n",
       " 'codegen': ['qkv_proj'],\n",
       " 'mistral': ['q_proj', 'v_proj'],\n",
       " 'mixtral': ['q_proj', 'v_proj'],\n",
       " 'stablelm': ['q_proj', 'v_proj'],\n",
       " 'phi': ['q_proj', 'v_proj', 'fc1', 'fc2'],\n",
       " 'gemma': ['q_proj', 'v_proj'],\n",
       " 'gemma2': ['q_proj', 'v_proj'],\n",
       " 'qwen2': ['q_proj', 'v_proj']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import mapping\n",
    "from peft.utils import other\n",
    "\n",
    "print('model_type', model.config.model_type)\n",
    "print(model.peft_config['default'].target_modules)\n",
    "\n",
    "# 默认的 target module\n",
    "other.TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7916e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Alpaca examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c217a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- https://github.com/tatsu-lab/stanford_alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e23b14a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:18:14.592578Z",
     "start_time": "2023-06-09T15:18:14.586008Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "836ff05e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:18:15.408107Z",
     "start_time": "2023-06-09T15:18:15.396281Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/nlp_study/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/nlp_study/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=1.5,\n",
    "    # nucleus sampling\n",
    "    top_p=0.8,\n",
    "    num_beams=4,\n",
    ")\n",
    "\n",
    "def inference(instruction, input=None):\n",
    "    prompt = generate_prompt(instruction, input)\n",
    "    # print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "    for s in generation_output.sequences:\n",
    "        output = tokenizer.decode(s)\n",
    "        print(\"Response:\", output.split(\"### Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "374a2e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:18:48.136010Z",
     "start_time": "2023-06-09T15:18:17.029296Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Instruction:  tell me a joke about an animal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Why don't elephants go to the movies?\n",
      "\n",
      "Because they can't sit still for more than 20 minutes!</s>\n"
     ]
    }
   ],
   "source": [
    "inference(input(\"Instruction: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_study",
   "language": "python",
   "name": "nlp_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}