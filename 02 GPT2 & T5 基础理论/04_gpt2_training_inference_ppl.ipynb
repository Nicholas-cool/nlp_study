{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a753963-40a8-4e2d-a10f-bfb31fc07af8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 引言\n",
    "\n",
    "1. 现代式语言模型，或者现代式人工智能最最核心的是 Transformer 架构，Transformer 架构最特色底层的计算机制是 Attention；\n",
    "2. 在 Transformer 架构上，在 Attention 计算上花再多的时间探索都是值得的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb035b3-f536-427a-b551-82459cf757f3",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efb58c6f4d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc43c8-60f1-4644-977e-bddcb37d9a2c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Casual/Decoder only 单向注意力的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428c821-a705-40f1-af86-2c615bff24c5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- BERT：双向注意力（bidirectional self attention）\n",
    "\n",
    "    $$\n",
    "    \\quad \\text{Attention}(Q^{(n \\times d_k)}, K^{(n \\times d_k)}, V^{(n \\times d_v)}) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \n",
    "    $$\n",
    "\n",
    "- GPT：单向因果注意力（causal self attention）\n",
    "\n",
    "    $$\n",
    "    \\quad \\text{Attention}(Q^{(n \\times d_k)}, K^{(n \\times d_k)}, V^{(n \\times d_v)}) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}+ M\\right)V\n",
    "    $$\n",
    "\n",
    "    - $M_{ij}=0, j\\ge i$\n",
    "    - $M_{ij}=1, j\\leq i$\n",
    "    \n",
    "    $$\n",
    "    M = \\begin{pmatrix}\n",
    "    1 & -\\infty & -\\infty & \\cdots & -\\infty \\\\\n",
    "    1 & 1 & -\\infty & \\cdots & -\\infty \\\\\n",
    "    1 & 1 & 1 & \\cdots & -\\infty \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & 1 & 1 & \\cdots & 1\n",
    "    \\end{pmatrix}_{n\\times n}\n",
    "    $$\n",
    "\n",
    "- T5：encoder 输出 K/V（取值相同），decoder 输出 Q，两者做 Cross attention\n",
    "\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    \\text{Encoder Self-Attention} &: \\quad \\text{Attention}(Q^{(n \\times d_k)}, K^{(n \\times d_k)}, V^{(n \\times d_v)}) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\\\\n",
    "    \\text{Decoder Masked Self-Attention} & : \\quad \\text{Attention}(Q^{(m \\times d_k)}, K^{(m \\times d_k)}, V^{(m \\times d_v)}) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}+M\\right)V \\\\\n",
    "    \\text{Cross-Attention} & : \\quad \\text{Attention}(Q^{(m \\times d_k)}, K^{(n \\times d_k)}, V^{(n \\times d_v)}) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\\\\n",
    "    \\end{split}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e1c31-0c59-46ff-86a6-dfc4610d18ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training & Inference/Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e600256-37fc-4913-b4b2-61360be1cc05",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- llama2/3 inference code: autoregressive, token by token generation\n",
    "    - https://github.com/meta-llama/llama3/blob/main/llama/generation.py#L179-L192C13\n",
    "    - 天然隐式地存在一个mask matrix\n",
    "    - 第一个单词，预测第二个单词，\n",
    "    - 第一个单词+第二个单词 => 预测第三个单词\n",
    "    - ...\n",
    "- training 的时候，因为有 casual mask（下三角矩阵的存在），等价于 autoregressive，token by token\n",
    "    - 显式地加 mask matrix，不让模型看到后边的结果\n",
    "- 计算 PPL （语言模型训练好坏的一个指标）的过程就是已有文本的测试集，可以用 casual mask 的方式实现自注意力，实现 autoregressive，token by token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41852fd1-55ab-4322-ab97-3e4b533a4ab1",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 初始化模型和 tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to('cuda')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# 输入序列\n",
    "input_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d47dc47-9223-43da-b535-312c50e22c6d",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149682a3-92d3-4d0b-b71a-10d17419a22f",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 50257]),\n",
       " tensor([[[-62.3139, -61.5645, -66.4938,  ..., -68.1286, -68.3228, -63.5829],\n",
       "          [-66.3240, -66.7452, -72.1618,  ..., -75.1955, -73.4651, -68.1786],\n",
       "          [-88.2910, -88.7236, -93.4422,  ..., -98.6212, -90.6379, -90.9913],\n",
       "          ...,\n",
       "          [-80.7563, -82.8596, -87.4034,  ..., -91.0716, -89.5648, -84.5701],\n",
       "          [-94.8247, -94.5054, -97.7886,  ..., -97.1508, -98.4995, -96.5095],\n",
       "          [-88.8787, -87.6110, -92.3262,  ..., -95.8310, -93.5164, -91.9581]]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方式一：model() 内部使用 attention_mask\n",
    "outputs = model(input_ids.to('cuda'), )\n",
    "logits = outputs.logits\n",
    "logits.shape, logits[:, 1:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ba13a7-0023-4230-a1d1-262bb691f930",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 方式二：逐步生成每个 token，并输出每一步的 logits\n",
    "generated_logits = []\n",
    "\n",
    "# 从第一个 token 开始逐步生成\n",
    "for i in range(1, input_ids.size(1)):\n",
    "    step_input_ids = input_ids[:, :i]  # 当前步骤的输入序列\n",
    "    outputs = model(step_input_ids.to('cuda'))\n",
    "    logits = outputs.logits\n",
    "    next_token_logits = logits[:, -1, :]  # 获取最后一个 token 的 logits\n",
    "    generated_logits.append(next_token_logits)\n",
    "\n",
    "generated_logits = torch.stack(generated_logits, dim=1)[:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1716ce98-da13-4ae2-b8f2-0c6cebdb902b",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 50257]),\n",
       " tensor([[[-62.3139, -61.5645, -66.4938,  ..., -68.1286, -68.3228, -63.5830],\n",
       "          [-66.3240, -66.7452, -72.1618,  ..., -75.1955, -73.4651, -68.1786],\n",
       "          [-88.2910, -88.7236, -93.4422,  ..., -98.6211, -90.6379, -90.9913],\n",
       "          ...,\n",
       "          [-80.7563, -82.8596, -87.4034,  ..., -91.0716, -89.5648, -84.5701],\n",
       "          [-94.8247, -94.5054, -97.7886,  ..., -97.1508, -98.4995, -96.5095],\n",
       "          [-88.8787, -87.6110, -92.3262,  ..., -95.8310, -93.5164, -91.9581]]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_logits.shape, generated_logits[:, 1:, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_study",
   "language": "python",
   "name": "nlp_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}