{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae92c1f-b702-4458-9930-8b549e606a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10f8f1",
   "metadata": {},
   "source": [
    "##  ÂõûÈ°æÔºöÊçüÂ§±ÂáΩÊï∞ËÆ°ÁÆóÂÖ¨Âºè"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82b0ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:28:17.611526Z",
     "start_time": "2023-07-08T13:28:17.588817Z"
    }
   },
   "source": [
    "- $q(x)$Ôºöfrom student modelÔºå$p(x)$Ôºöfrom teacher model\n",
    "- ÂÖ∂Ê¨°ÂØπ‰∫é $q(x), p(x)$ Âú®ËÆ°ÁÆóÊó∂ÈúÄË¶ÅÂä†Ê∏©Â∫¶\n",
    "$$\n",
    "\\begin{split}\n",
    "L_{\\text{student}}&=\\alpha L_{\\text{CE}} + (1-\\alpha)L_{KD}\\\\\n",
    "&=\\alpha L_{\\text{CE}} + (1-\\alpha)T^2D_{KL}\\\\\n",
    "&=\\alpha L_{\\text{CE}} + (1-\\alpha)T^2\\sum_ip_i(x)\\log\\frac{p_i(x)}{q_i(x)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- ÂÖ∂‰∏≠ÔºåKL Êï£Â∫¶ÂèØ‰ª•ÈÄöËøá `nn.KLDivLoss()` ÂáΩÊï∞Êù•ËÆ°ÁÆó\n",
    "    - inputs ($q(x)$): log probabilities\n",
    "    - labels ($p(x)$): normal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede970ba",
   "metadata": {},
   "source": [
    "## TrainerArguments & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf76fa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:42:34.579255Z",
     "start_time": "2023-07-08T13:42:34.573911Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59dde685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:42:36.134423Z",
     "start_time": "2023-07-08T13:42:36.126195Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DistillTrainingArguments(TrainingArguments):\n",
    "    # TrainingArguments: @dataclass\n",
    "    # Â¢ûÂä†‰∏§‰∏™ KD ÊâÄÈúÄÁöÑÂèÇÊï∞ÂèÇÊï∞\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2., **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35763030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:45:33.019991Z",
     "start_time": "2023-07-08T13:45:33.005716Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DistillTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        # Â¢ûÂä† teacher_model ÂèÇÊï∞\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "\n",
    "    # ÈáçÂÜô trainer ‰∏≠Ê†∏ÂøÉÊñπÊ≥ïÔºàforward ËÆ°ÁÆóÊçüÂ§±Ôºâ\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        s_output = model(**inputs)\n",
    "        s_ce = s_output.loss\n",
    "        s_logits = s_output.logits\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            t_output = self.teacher_model(**inputs)\n",
    "            t_logits = t_output.logits\n",
    "        \n",
    "        loss_kl_fct = nn.KLDivLoss(reduction='batchmean')\n",
    "        loss_kd = self.args.temperature**2 * loss_kl_fct(F.log_softmax(s_logits/self.args.temperature, dim=-1),\n",
    "                                                         F.softmax(t_logits/self.args.temperature, dim=-1))\n",
    "        loss = self.args.alpha * s_ce + (1 - self.args.alpha) * loss_kd\n",
    "        return (loss, s_output) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd85c3",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2702dd",
   "metadata": {},
   "source": [
    "### datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f683fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:45:41.732252Z",
     "start_time": "2023-07-08T13:45:41.725945Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b79b20b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:45:54.620037Z",
     "start_time": "2023-07-08T13:45:42.507852Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# SequenceClassification\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c17cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:45:57.176014Z",
     "start_time": "2023-07-08T13:45:57.167301Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'intent'],\n",
       "        num_rows: 15250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'intent'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'intent'],\n",
       "        num_rows: 5500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f258185",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:46:17.612259Z",
     "start_time": "2023-07-08T13:46:17.599359Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['what expression would i use to say i love you if i were an italian',\n",
       "  \"can you tell me how to say 'i do not speak much spanish', in spanish\",\n",
       "  \"what is the equivalent of, 'life is good' in french\",\n",
       "  \"tell me how to say, 'it is a beautiful morning' in italian\",\n",
       "  'if i were mongolian, how would i say that i am a tourist',\n",
       "  \"how do i say 'hotel' in finnish\",\n",
       "  \"i need you to translate the sentence, 'we will be there soon' into portuguese\",\n",
       "  'please tell me how to ask for a taxi in french',\n",
       "  \"can you tell me how i would say, 'more bread please' in french\",\n",
       "  \"what is the correct way to say 'i am a visitor' in french\"],\n",
       " 'intent': [61, 61, 61, 61, 61, 61, 61, 61, 61, 61]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinc['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4366a862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:46:29.621894Z",
     "start_time": "2023-07-08T13:46:29.611922Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents = clinc['train'].features['intent']\n",
    "num_labels = intents.num_classes\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4a0a9",
   "metadata": {},
   "source": [
    "### Student model ÂàùÂßãÂåñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "313668df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:46:48.960423Z",
     "start_time": "2023-07-08T13:46:48.949623Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a68a82b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:47:49.353462Z",
     "start_time": "2023-07-08T13:47:00.143015Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Â≠¶ÁîüÊ®°Âûã\n",
    "s_ckpt = 'distilbert-base-uncased'\n",
    "s_tokenizer = AutoTokenizer.from_pretrained(s_ckpt)\n",
    "\n",
    "# ÊïôÂ∏àÊ®°Âûã\n",
    "t_ckpt = 'transformersbook/bert-base-uncased-finetuned-clinc'\n",
    "t_model = AutoModelForSequenceClassification.from_pretrained(t_ckpt, num_labels=num_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30765bdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:48:33.125442Z",
     "start_time": "2023-07-08T13:48:32.121377Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 15250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinc_enc = clinc.map(\n",
    "    lambda batch: s_tokenizer(batch['text'], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "clinc_enc = clinc_enc.rename_columns({'intent': 'labels'})\n",
    "clinc_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9a08ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:50:12.756586Z",
     "start_time": "2023-07-08T13:50:12.379492Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/nlp_study/lib/python3.8/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "s_training_args = DistillTrainingArguments(\n",
    "    output_dir='distilbert-base-uncased-ft-clinc', \n",
    "    evaluation_strategy='epoch', \n",
    "    num_train_epochs=5, \n",
    "    learning_rate=3e-4, \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size, \n",
    "    alpha=0.5, \n",
    "    weight_decay=0.01, \n",
    "    logging_strategy='epoch',\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"  # ËøôÊ†∑Â∞±‰∏ç‰ºöËá™Âä®ÂêØÁî® W&B\n",
    ")\n",
    "\n",
    "s_config = AutoConfig.from_pretrained(\n",
    "    s_ckpt, \n",
    "    num_labels=num_labels, \n",
    "    id2label=t_model.config.id2label, \n",
    "    label2id=t_model.config.label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86ba1a09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:50:32.717059Z",
     "start_time": "2023-07-08T13:50:32.709207Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def student_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(s_ckpt, config=s_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294067e",
   "metadata": {},
   "source": [
    "## Student model ËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8026a1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:50:45.557236Z",
     "start_time": "2023-07-08T13:50:41.094296Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy_score = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d9f99a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:51:05.438475Z",
     "start_time": "2023-07-08T13:51:05.430725Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ÂáÜÁ°ÆÂ∫¶ÊåáÊ†áËÆ°ÁÆóÂáΩÊï∞\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return accuracy_score.compute(references=labels, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c635f0b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:54:04.344304Z",
     "start_time": "2023-07-08T13:51:39.054134Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1195' max='1195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1195/1195 02:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.239600</td>\n",
       "      <td>0.446824</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.331388</td>\n",
       "      <td>0.932258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.223600</td>\n",
       "      <td>0.299509</td>\n",
       "      <td>0.945806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.283754</td>\n",
       "      <td>0.950645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>0.276170</td>\n",
       "      <td>0.951613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1195, training_loss=0.4302276834783195, metrics={'train_runtime': 142.9024, 'train_samples_per_second': 533.581, 'train_steps_per_second': 8.362, 'total_flos': 427022126020140.0, 'train_loss': 0.4302276834783195, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_trainer = DistillTrainer(\n",
    "    model_init=student_init, \n",
    "    teacher_model=t_model, \n",
    "    args=s_training_args, \n",
    "    train_dataset=clinc_enc['train'], \n",
    "    eval_dataset=clinc_enc['validation'], \n",
    "    compute_metrics=compute_metrics, \n",
    "    tokenizer=s_tokenizer,\n",
    ")\n",
    "distill_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f6398eb-7094-4dc4-a333-e0bc8235f0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distill_trainer.save_model(\"./distilbert-base-uncased-ft-clinc\")  # ‰øùÂ≠òÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "820b9f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:54:24.687818Z",
     "start_time": "2023-07-08T13:54:24.678385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil(15250 / (64 * 1)) * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c3a00",
   "metadata": {},
   "source": [
    "## Student model ‰ΩøÁî®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6a527d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:55:04.861128Z",
     "start_time": "2023-07-08T13:55:03.957835Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ft_ckpt = 'lanchunhui/distilbert-base-uncased-ft-clinc'\n",
    "# distill_trainer.push_to_hub('finetune completed!')\n",
    "\n",
    "pipe = pipeline('text-classification', model='./distilbert-base-uncased-ft-clinc/', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df7cb41a-6140-4395-a476-e55bd8482b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'car_rental', 'score': 0.8687736988067627}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"\"\" Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need a 15 passenger van \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_study",
   "language": "python",
   "name": "nlp_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
